<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.ico">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"zjuturtle.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":{"gitalk":null}},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="现在手头上接触的很多线上功能都使用了 transformer 这种深度学习的模型（手头上的 ASR 和 TTS 都不约而同的用了），虽然我只做工程的实现（对我就是调 API 的），但也还是想稍微深入学习一下 transformer 相关知识。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 模型笔记">
<meta property="og:url" content="https://zjuturtle.com/2020/01/25/transformer/index.html">
<meta property="og:site_name" content="zjuturtle&#39;s blog">
<meta property="og:description" content="现在手头上接触的很多线上功能都使用了 transformer 这种深度学习的模型（手头上的 ASR 和 TTS 都不约而同的用了），虽然我只做工程的实现（对我就是调 API 的），但也还是想稍微深入学习一下 transformer 相关知识。">
<meta property="og:locale">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/The_transformer_encoders_decoders.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/The_transformer_encoder_decoder_stack.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/Transformer_encoder.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_self-attention_visualization.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_self_attention_vectors.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_self_attention_score.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/self-attention_softmax.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/self-attention-output.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/self-attention-matrix-calculation.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/self-attention-matrix-calculation-2.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_attention_heads_qkv.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_attention_heads_weight_matrix_o.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_multi-headed_self-attention-recap.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_self-attention_visualization_2.png">
<meta property="og:image" content="https://zjuturtle.com/2020/01/25/transformer/transformer_resideual_layer_norm.png">
<meta property="article:published_time" content="2020-01-25T10:59:44.000Z">
<meta property="article:modified_time" content="2021-10-26T12:02:47.650Z">
<meta property="article:author" content="zjuturtle">
<meta property="article:tag" content="ASR">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="TTS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zjuturtle.com/2020/01/25/transformer/The_transformer_encoders_decoders.png">


<link rel="canonical" href="https://zjuturtle.com/2020/01/25/transformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"https://zjuturtle.com/2020/01/25/transformer/","path":"2020/01/25/transformer/","title":"Transformer 模型笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Transformer 模型笔记 | zjuturtle's blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">zjuturtle's blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-number">1.</span> <span class="nav-text">前置知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sequence-to-sequence-%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">sequence to sequence 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.</span> <span class="nav-text">Attention 机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer"><span class="nav-number">2.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention"><span class="nav-number">2.1.</span> <span class="nav-text">self-attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%80"><span class="nav-number">2.1.1.</span> <span class="nav-text">步骤一</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4%E4%BA%8C"><span class="nav-number">2.1.2.</span> <span class="nav-text">步骤二</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4%E4%B8%89"><span class="nav-number">2.1.3.</span> <span class="nav-text">步骤三</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4%E5%9B%9B"><span class="nav-number">2.1.4.</span> <span class="nav-text">步骤四</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="nav-number">2.1.5.</span> <span class="nav-text">矩阵化操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-headed-attention"><span class="nav-number">2.2.</span> <span class="nav-text">multi-headed attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">3.</span> <span class="nav-text">位置编码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="nav-number">4.</span> <span class="nav-text">残差连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder-%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">Decoder 网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="zjuturtle"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">zjuturtle</p>
  <div class="site-description" itemprop="description">瞎几把搞的幸运E</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zjuturtle" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zjuturtle" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:leijinghaog@gmail.com" title="E-Mail → mailto:leijinghaog@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://zjuturtle.com/2020/01/25/transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="zjuturtle">
      <meta itemprop="description" content="瞎几把搞的幸运E">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="zjuturtle's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer 模型笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-01-25 18:59:44" itemprop="dateCreated datePublished" datetime="2020-01-25T18:59:44+08:00">2020-01-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-10-26 20:02:47" itemprop="dateModified" datetime="2021-10-26T20:02:47+08:00">2021-10-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>现在手头上接触的很多线上功能都使用了 transformer 这种深度学习的模型（手头上的 ASR 和 TTS 都不约而同的用了），虽然我只做工程的实现（对我就是调 API 的），但也还是想稍微深入学习一下 transformer 相关知识。</p>
<span id="more"></span>
<h2 id="前置知识">前置知识</h2>
<h3 id="sequence-to-sequence-模型">sequence to sequence 模型</h3>
<p>其实这个比较好解释，指的是模型的输入是一个序列信息（可以是文本，音频，视频etc），输出也是序列信息。典型的如翻译模型，从中文翻译到英文。</p>
<p>一般地，Sequence to sequence 模型会由一个 encoder 和 decoder 组成。他们都是 RNN (或者类似结构的)模型。处理模式大致为:输入 -&gt; encoder -&gt; context(向量) -&gt; decoder -&gt; 输出。context 向量长度可能是 256/512/1024 等。</p>
<p>注意，一般而言 Sequence to sequence 的模型的 encoder 在运行时会维护一个 hidden state 的信息，但这个信息不会直接传递给 decoder。实际上输出信息给到 encoder 之后，需要等待 encoder 的 RNN 网络处理完得到 context, 再把 context 传递给 decoder。</p>
<h3 id="attention-机制">Attention 机制</h3>
<p>在很多地方都提到了《Attention is all you need》这一篇文章，算是最初的源头。Attention的思想理解起来比较容易，就是在 decoding 阶段对input中的信息赋予不同权重。在nlp中就是针对 sequence 的每个time step input，在cv中就是针对每个pixel。</p>
<p>思想简单，具体到实现上就有点麻烦了。我看了很多参考资料（大部分中文资料都是互相抄一抄），然而还是云里雾里。</p>
<p>简单而言，Attention 就是我们把 encoder 中的每一次 hidden state 都传给 decoder，同时 decoder 根据当前的输入位置去对 hidden state 进行加权平均的到当前注意信息，连同 context 一并输入。</p>
<h2 id="transformer">Transformer</h2>
<p>Attention 机制帮助提升了机器翻译的质量，transformer 模型则让模型训练可以并行化，大大提升训练效率。我们以翻译为例子说明 transformer 模型。</p>
<p>典型的 sequence to sequence 如下图</p>
<img src="/2020/01/25/transformer/The_transformer_encoders_decoders.png" class="">
<p>在文献中 encoders 是由6个子网络层叠而来。每个网络的结构是一样的（当然它们的权值并不共享）。decoder 同理。注意到 encoder 的网络的最后一次输出（其实是下文中出现的 K 和 V ）输入到每一层 decoder 中。</p>
<img src="/2020/01/25/transformer/The_transformer_encoder_decoder_stack.png" class="">
<p>对于一个 encoder 的子网络，它由 self-attention 和 feed forward 两层组成；对于一个 decoder 的子网络，它由 self-attention，encoder-decoder attention，feed forward 三层组成</p>
<ul>
<li>self-attention：当前翻译和已经翻译的前文之间的关系</li>
<li>encoder-decoder attention：当前翻译和编码的特征向量之间的关系</li>
</ul>
<img src="/2020/01/25/transformer/Transformer_encoder.png" class="">
<p>对于 encoder 的输入，它其实是结果 embedding algorithm 得到的词向量（实际上是把词映射成一个高维向量，注意这里并不是简单的 index，这里具体不展开了）。词向量的长度会取大于我们需要翻译句子的最大长度的2的幂（不是很理解为什么？）。</p>
<h3 id="self-attention">self-attention</h3>
<p>Self-attention 指的是句子中的某个词和其他部分的相关程度。比如下面这句话</p>
<blockquote>
<p>The animal didn't cross the street because it was too tired</p>
</blockquote>
<p>句子中的 it 明显指代 animal，但对于机器来说，这就比较困难了：it 是指代 animal 还是 street？Self-attention 就是解决这个问题的方法——计算 it 和句子中其他词的相关度。</p>
<img src="/2020/01/25/transformer/transformer_self-attention_visualization.png" class="">
<h4 id="步骤一">步骤一</h4>
<p>从 encoder 的每一个输入（即一个个 embedding 后的词向量）构造 3 个向量，文献中称之为 Query/Key/Value vector。这三个向量实际上是将词向量和 <span class="math inline">\(W^Q\)</span>，<span class="math inline">\(W^K\)</span>，<span class="math inline">\(W^V\)</span> 矩阵相乘得到的（这3个矩阵是训练过程中训练得到的，不过这个是怎么训练的呢？）。注意到 Query/Key/Value vector 的维度是一致的，但是不一定需要和词向量的维度一致。一般来说为了计算效率，Query/Key/Value vector 的维度会低于词向量的维度，比如词向量512维，QKV 64维。</p>
<img src="/2020/01/25/transformer/transformer_self_attention_vectors.png" class="">
<h4 id="步骤二">步骤二</h4>
<p>计算单个词（词向量）相对于整个句子的 score。比如第一个词 Thinking，它在位置 1 的分数是 <span class="math inline">\(q_1 \cdot k_1\)</span>，位置 2 的分数是 <span class="math inline">\(q_1 \cdot k_2\)</span>，以此类推。</p>
<img src="/2020/01/25/transformer/transformer_self_attention_score.png" class="">
<h4 id="步骤三">步骤三</h4>
<p>对计算得到的分数除以 8，即 QKV 维度的开平方 <span class="math inline">\(\sqrt{64}=8\)</span>，据说是为了有更好的梯度，我感觉更像是拍脑瓜想出来的注意。然后再对这些 score 进行 softmax。</p>
<img src="/2020/01/25/transformer/self-attention_softmax.png" class="">
<h4 id="步骤四">步骤四</h4>
<p>现在要用到 Value 向量了。处理也很简单，就是将上面计算得到的 softmax 乘上 value 向量，再统统累加在一起，得到最终的该词的 self-attention 值。即 <span class="math inline">\(z_1 = v_1softmax_1+v_2softmax_2\)</span>。注意这里的 <span class="math inline">\(z_2\)</span> 指代的是第一个词 Thinking 在第二个位置的 self-attention 值。</p>
<img src="/2020/01/25/transformer/self-attention-output.png" class="">
<h4 id="矩阵化操作">矩阵化操作</h4>
<p>在最开头我们提到过 Transformer 模型可以很好地并行化，实际上是用矩阵乘法得到。上述 self-attention 矩阵化计算如下。<span class="math inline">\(X\)</span> 的每一行都是代表一句话中的一个词向量</p>
<img src="/2020/01/25/transformer/self-attention-matrix-calculation.png" class="">
<img src="/2020/01/25/transformer/self-attention-matrix-calculation-2.png" class="">
<h3 id="multi-headed-attention">multi-headed attention</h3>
<p>文章中扩展了 self-attention 的概念为 multi-headed attention</p>
<ol type="1">
<li>它扩展了模型专注在不同位置能力。在上面的 self-attention 例子中，<span class="math inline">\(z_1\)</span> 几乎不包含其他位置的词的信息（实际上只有 <span class="math inline">\(k\)</span> ），所以可以认为 <span class="math inline">\(z_1\)</span> 受到词本身的影响很大。（我其实没太理解这一点）</li>
<li>它赋予了注意力层多个表达子空间。这个很明显，实际上它用了多组 QKV，自然表征空间更大。</li>
</ol>
<p>multi-headed attention 比较简单，就是把 self-attention 拷贝8份（当然权值不一样），然后把每个 head 得到的 self-attention 连接起来，最后将这个连起来的向量乘以一个训练好的矩阵 <span class="math inline">\(W^O\)</span>，得到的结果我们称它为 <span class="math inline">\(Z\)</span> 。</p>
<img src="/2020/01/25/transformer/transformer_attention_heads_qkv.png" class="">
<img src="/2020/01/25/transformer/transformer_attention_heads_weight_matrix_o.png" class="">
<img src="/2020/01/25/transformer/transformer_multi-headed_self-attention-recap.png" class="">
<p>然后还是看回 The animal didn't cross the street because it was too tired 这句话。里面的两个 head 的 self-attention 在 it 的位置如下。这里有个小问题，每一个 head 的 self-attention 应该是一个向量，但图中直接用了一个色块表示，是直接取向量的模？</p>
<img src="/2020/01/25/transformer/transformer_self-attention_visualization_2.png" class="">
<h2 id="位置编码">位置编码</h2>
<p>在输入端光是有词向量（word embeddings）是不够的，我们还需要每一次输入的位置信息。一个简单的方法是在输入的向量前增加位置编码向量。</p>
<p>位置编码向量有很多种，文献中用的是三角函数生成。用三角函数的好处在于理论上它可以在有限的维度中表示任意长度句子的相对位置。实际上，只要位置编码可以表示词在句子中的相对位置就可以，并不一定需要使用三角函数。</p>
<h2 id="残差连接">残差连接</h2>
<p>Transformer 的网络里还有个小细节需要注意，在每一个子层里（包括 self-attention，feed forward）都有一个残差连接（主要是为了解决梯度爆炸/梯度消散问题）。如下图就是 encoder 里的残差连接。</p>
<img src="/2020/01/25/transformer/transformer_resideual_layer_norm.png" class="">
<h2 id="decoder-网络">Decoder 网络</h2>
<p>Encoder 从输入一个序列开始，encoder 的最终输出是一系列的 attention vector K 和 V</p>
<p>Decoder 的 self-attention 层和 encoder 的有些许不同。decoder 的 self-attention 层只允许关联之前的位置（通过在 softmax 之前增加一个 -inf 的 mask 实现）。Decoder 中的 Encoder-decoder attention 层的计算方式和 self-attention 类似，但是它的 Queries 矩阵来自于它的前一层，而 Keys 和 Values 矩阵则来自于 encoder 的输出。</p>
<p>最后，在 decoder 网络的后面挂一个 softmax 就可以得到最终的词结果。</p>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47063917">https://zhuanlan.zhihu.com/p/47063917</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47282410">https://zhuanlan.zhihu.com/p/47282410</a></p>
<p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">这个很赞https://jalammar.github.io/illustrated-transformer/</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ASR/" rel="tag"># ASR</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/TTS/" rel="tag"># TTS</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/01/25/intellij-docker/" rel="prev" title="使用 Intellij 调试 Docker 内的 Java 代码">
                  <i class="fa fa-chevron-left"></i> 使用 Intellij 调试 Docker 内的 Java 代码
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/01/25/jni-multithread-callback/" rel="next" title="JNI 的多线程回调">
                  JNI 的多线程回调 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zjuturtle</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"AnotherZjuturtle","repo":"Gitalk","client_id":"08e86c9d0536ed791bf9","client_secret":"0982e8cb7b507eee3022547b44ebc4c4f6f2a944","admin_user":"AnotherZjuturtle","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"0f9cb79821b2ef164224803b1c7c229f"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
